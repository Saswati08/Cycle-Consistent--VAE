{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "full_q2.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "W6-atapMHsUP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive/')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pl3b6FRoILjX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "cd drive/My Drive/DL_3/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4QcV1DGOYG83",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install tensorboardx"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c2ow_zAFoot-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import random\n",
        "import numpy as np\n",
        "from itertools import cycle\n",
        "from torchvision import datasets\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torchvision import datasets\n",
        "from torch.autograd import Variable\n",
        "from torch.utils.data import DataLoader\n",
        "import h5py\n",
        "from itertools import cycle\n",
        "from collections import OrderedDict\n",
        "from scipy.io import loadmat\n",
        "import glob\n",
        "import os\n",
        "\n",
        "import torch.optim as optim\n",
        "from torch.autograd import Variable\n",
        "from tensorboardX import SummaryWriter\n",
        "from torch.utils.data import DataLoader\n",
        "import matplotlib.pyplot as plt\n",
        "from torch.autograd import Variable\n",
        "from mpl_toolkits.axes_grid1 import ImageGrid\n",
        "from torchvision.transforms import Compose, ToTensor\n",
        "from collections import defaultdict"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sOWJqj5hovve",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "filenames = glob.glob('data/sprites/*.mat')\n",
        "filenames = sorted(filenames)\n",
        "split_file = filenames[len(filenames) - 1]\n",
        "print(split_file)\n",
        "filenames = filenames[0 : len(filenames) - 2]\n",
        "print(filenames)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ue-EWyEMcAq3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(split_file)\n",
        "f = loadmat(split_file)\n",
        "print(f.keys())\n",
        "print(f['trainidx'][0])\n",
        "print(f['testidx'][0])\n",
        "print(f['validx'][0])\n",
        "# print(f['sprites'].shape)\n",
        "# for j in range(0, 21):\n",
        "#   print(np.shape(f[f.get('sprites')[j][0]].value))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CkuMwnf0cCQg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_dict = defaultdict()\n",
        "train_dict = { i : 1 for i in f['trainidx'][0]}\n",
        "print(train_dict)\n",
        "test_dict = defaultdict()\n",
        "test_dict = {i : 1 for i in f['testidx'][0]}\n",
        "print(test_dict)\n",
        "valid_dict = defaultdict()\n",
        "valid_dict = {i : 1 for i in f['validx'][0]}\n",
        "print(valid_dict)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cYUVNMCVcHKV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_filenames = []\n",
        "test_filenames = []\n",
        "valid_filenames = []\n",
        "file_list = []\n",
        "c = 0\n",
        "for i in filenames:\n",
        "  sts = i[-7: -4]\n",
        "  # print(str)\n",
        "  if sts[0] >= 'a' and sts[0] <= 'z':\n",
        "    # print(str[2:])\n",
        "    sts = int(sts[2:])\n",
        "  elif sts[0] == '_':\n",
        "    # print(str[1:])\n",
        "    sts = int(sts[1 :])\n",
        "  else:\n",
        "    sts = int(sts)\n",
        "  # print(str)\n",
        "  file_list.append(sts)\n",
        "\n",
        "for i in file_list:\n",
        "  if i in train_dict:\n",
        "    train_filenames.append(filenames[c])\n",
        "  elif i in test_dict:\n",
        "    test_filenames.append(filenames[c])\n",
        "  elif i in valid_dict:\n",
        "    valid_filenames.append(filenames[c])\n",
        "  else:\n",
        "    print(type(i), type(train_dict[585]))\n",
        "  c += 1\n",
        "print(c)\n",
        "print(len(train_filenames))\n",
        "print(len(test_filenames))\n",
        "print(len(valid_filenames))\n",
        "  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "alSDfgK_piQs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def construct_dictionary(filenames):\n",
        "  num = 0\n",
        "  data_dict = {}\n",
        "  for i in filenames:\n",
        "    f = h5py.File(i, 'r')\n",
        "    # print(np.shape(f.get('sprites')))\n",
        "    # arr = np.reshape(f[f.get('sprites')[4][0]].value,(7, 3, 60, 60))\n",
        "    for j in range(0, 21):\n",
        "      # print(np.shape(f[f.get('sprites')[j][0]].value))\n",
        "      k = np.shape(f[f.get('sprites')[j][0]].value)[0]\n",
        "      arr = np.reshape(f[f.get('sprites')[j][0]].value,(k, 3, 60, 60))\n",
        "      if num not in data_dict:\n",
        "        data_dict[num] =  arr\n",
        "      else:\n",
        "        arr2 = data_dict[num]\n",
        "        arr2 = np.concatenate([arr2, arr], 0)\n",
        "        data_dict[num] = arr2\n",
        "\n",
        "    num += 1\n",
        "  return data_dict"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vMTA_o3_cpqn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data_dict = construct_dictionary(train_filenames[0:100])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sZ03Se5KjGT1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# import pickle\n",
        "# a_file = open(\"train_dict.pkl\", \"wb\")\n",
        "# pickle.dump(data_dict, a_file)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VQb0Z-thc3Jq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_data_dict = construct_dictionary(test_filenames)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CB2w1Wi3mtJO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(len(data_dict))\n",
        "print(len(test_data_dict))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OqQ-4Bckkkic",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# import pickle\n",
        "# a_file = open(\"test_dict.pkl\", \"wb\")\n",
        "# pickle.dump(test_data_dict, a_file)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7L3hKW9-rDbY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "a_file = open(\"train_dict.pkl\", \"rb\")\n",
        "data_dict = pickle.load(a_file)\n",
        "\n",
        "a_file = open(\"test_dict.pkl\", \"rb\")\n",
        "test_data_dict = pickle.load(a_file)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LEhc1K-fTSTW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# for keys in data_dict:\n",
        "#   print(np.shape(data_dict[keys]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5Wd6SiVh53-M",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "from torch.utils import data\n",
        "\n",
        "class Dataset():\n",
        "  'Characterizes a dataset for PyTorch'\n",
        "  def __init__(self, data_dict, num_samples):\n",
        "        'Initialization'\n",
        "        self.data_dict = data_dict\n",
        "        self.num_samples = num_samples\n",
        "  def __len__(self):\n",
        "        'Denotes the total number of samples'\n",
        "        return self.num_samples\n",
        "\n",
        "  def __getitem__(self, index):\n",
        "        'Generates one sample of data'\n",
        "        # Select sample\n",
        "        # key is class\n",
        "        key = (index // 178)\n",
        "        # num_img is image number for that class \n",
        "        num_img = (index // 100)\n",
        "        # print(key)\n",
        "        arr = self.data_dict[key]\n",
        "        image = arr[num_img]\n",
        "        label = key\n",
        "        return image, random.SystemRandom().choice(self.data_dict[key]), label\n",
        "\n",
        "dataset = Dataset(data_dict, 178 * 100)\n",
        "loader = cycle(DataLoader(dataset, batch_size=16, shuffle = True, num_workers=0, drop_last=True))\n",
        "print(dataset.data_dict.keys())\n",
        "\n",
        "image_batch, image_batch_2, labels_batch = next(loader)\n",
        "print(labels_batch)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HqmAK6kpdCN8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class test_Dataset():\n",
        "  'Characterizes a dataset for PyTorch'\n",
        "  def __init__(self, data_dict, num_samples):\n",
        "        'Initialization'\n",
        "        self.data_dict = data_dict\n",
        "        self.num_samples = num_samples\n",
        "  def __len__(self):\n",
        "        'Denotes the total number of samples'\n",
        "        return self.num_samples\n",
        "\n",
        "  def __getitem__(self, index):\n",
        "        'Generates one sample of data'\n",
        "        # Select sample\n",
        "        key = (index // 178) \n",
        "        num_img = (index // 100)\n",
        "        # print(key)\n",
        "        arr = self.data_dict[key]\n",
        "        image = arr[num_img]\n",
        "        label = key\n",
        "        return image, random.SystemRandom().choice(self.data_dict[key]), label\n",
        "\n",
        "test_dataset = test_Dataset(test_data_dict, 178 * 100)\n",
        "test_loader = cycle(DataLoader(test_dataset, batch_size = 8, shuffle = True, num_workers=0, drop_last=True))\n",
        "print(dataset.data_dict.keys())\n",
        "\n",
        "image_batch, image_batch_2, labels_batch = next(test_loader)\n",
        "print(labels_batch)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pzqyArZ2srjY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Encoder(nn.Module):\n",
        "    def __init__(self, style_dim, class_dim):\n",
        "        super(Encoder, self).__init__()\n",
        "        \n",
        "        self.conv1 = nn.Conv2d(in_channels= 3, out_channels=16, kernel_size=5, stride=2, padding=1, bias=True)\n",
        "        self.conv_1_in = nn.InstanceNorm2d(num_features=16, track_running_stats=True)\n",
        "        self.relu_1 = nn.ReLU(inplace=True)\n",
        "\n",
        "        self.conv2 = nn.Conv2d(in_channels=16, out_channels=32, kernel_size=5, stride=2, padding=1, bias=True)\n",
        "        self.conv_2_in = nn.InstanceNorm2d(num_features=32, track_running_stats=True)\n",
        "        self.relu_2 = nn.ReLU(inplace=True)\n",
        "\n",
        "        self.conv3 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=5, stride=2, padding=1, bias=True)\n",
        "        self.conv_3_in = nn.InstanceNorm2d(num_features=64, track_running_stats=True)\n",
        "        self.relu_3 = nn.ReLU(inplace=True)\n",
        "\n",
        "        # Style embeddings\n",
        "        self.style_mu = nn.Linear(in_features= 2304, out_features=style_dim, bias=True)\n",
        "        self.style_logvar = nn.Linear(in_features= 2304, out_features=style_dim, bias=True)\n",
        "\n",
        "        # Class embeddings\n",
        "        self.class_output = nn.Linear(in_features=2304, out_features=class_dim, bias=True)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv1(x)\n",
        "        x = self.conv_1_in(x)\n",
        "        x = self.relu_1(x)\n",
        "        x = self.conv2(x)\n",
        "        x = self.conv_2_in(x)\n",
        "        x = self.relu_2(x)\n",
        "        x = self.conv3(x)\n",
        "        x = self.conv_3_in(x)\n",
        "        x = self.relu_3(x)\n",
        "        x = x.view(x.size(0), x.size(1) * x.size(2) * x.size(3))\n",
        "\n",
        "        style_embeddings_mu = self.style_mu(x)\n",
        "        style_embeddings_logvar = self.style_logvar(x)\n",
        "        class_embeddings = self.class_output(x)\n",
        "\n",
        "        return style_embeddings_mu, style_embeddings_logvar, class_embeddings\n",
        "\n",
        "class Decoder(nn.Module):\n",
        "    def __init__(self, style_dim, class_dim):\n",
        "        super(Decoder, self).__init__()\n",
        "\n",
        "        # Style embeddings input\n",
        "        self.style_input = nn.Linear(in_features=style_dim, out_features=2304, bias=True)\n",
        "\n",
        "        # Class embeddings input\n",
        "        self.class_input = nn.Linear(in_features=class_dim, out_features=2304, bias=True)\n",
        "\n",
        "        self.deconv1 = nn.ConvTranspose2d(in_channels=128, out_channels=32, kernel_size=4, stride=2, padding=0, bias=True)\n",
        "        self.deconv1_in = nn.InstanceNorm2d(num_features=32, track_running_stats=True)\n",
        "        self.deconv1_relu = nn.LeakyReLU(negative_slope=0.2, inplace=True)\n",
        "\n",
        "        self.deconv2 = nn.ConvTranspose2d(in_channels=32, out_channels=16, kernel_size=4, stride=2, padding=0, bias=True)\n",
        "        self.deconv2_in = nn.InstanceNorm2d(num_features= 16, track_running_stats=True)\n",
        "        self.deconv2_relu = nn.LeakyReLU(negative_slope=0.2, inplace=True)\n",
        "\n",
        "        self.deconv3 = nn.ConvTranspose2d(in_channels=16, out_channels= 3, kernel_size=4, stride=2, padding=1, bias=True)\n",
        "        self.sigmoid_output = nn.Sigmoid()\n",
        "       \n",
        "    def forward(self, style_embeddings, class_embeddings):\n",
        "        style_embeddings = F.leaky_relu_(self.style_input(style_embeddings), negative_slope=0.2)\n",
        "        class_embeddings = F.leaky_relu_(self.class_input(class_embeddings), negative_slope=0.2)\n",
        "\n",
        "        x = torch.cat((style_embeddings, class_embeddings), dim=1)\n",
        "        x = x.view(x.size(0), 128, 6, 6)\n",
        "        x = self.deconv1(x)\n",
        "        x = self.deconv1_in(x)\n",
        "        x = self.deconv1_relu(x)\n",
        "\n",
        "        x = self.deconv2(x)\n",
        "        x = self.deconv2_in(x)\n",
        "        x = self.deconv2_relu(x)\n",
        "\n",
        "        x = self.deconv3(x)\n",
        "        x = self.sigmoid_output(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "class Classifier(nn.Module):\n",
        "    def __init__(self, z_dim, num_classes):\n",
        "        super(Classifier, self).__init__()\n",
        "\n",
        "        self.fc_model = nn.Sequential(OrderedDict([\n",
        "            ('fc_1', nn.Linear(in_features=z_dim, out_features=256, bias=True)),\n",
        "            ('fc_1_bn', nn.BatchNorm1d(num_features=256)),\n",
        "            ('LeakyRelu_1', nn.LeakyReLU(negative_slope=0.2, inplace=True)),\n",
        "\n",
        "            ('fc_2', nn.Linear(in_features=256, out_features=256, bias=True)),\n",
        "            ('fc_2_bn', nn.BatchNorm1d(num_features=256)),\n",
        "            ('LeakyRelu_2', nn.LeakyReLU(negative_slope=0.2, inplace=True)),\n",
        "\n",
        "            ('fc_3', nn.Linear(in_features=256, out_features=num_classes, bias=True))\n",
        "        ]))\n",
        "\n",
        "    def forward(self, z):\n",
        "        x = self.fc_model(z)\n",
        "\n",
        "        return x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NDA6tmiHVc_X",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class ags():\n",
        "  def __init__(self):\n",
        " \n",
        "# add arguments\n",
        "  \n",
        "    self.cuda = True\n",
        "    self.batch_size = 16\n",
        "    self.num_channels = 3\n",
        "    self.initial_learning_rate = 0.0001\n",
        "    self.style_dim = 64\n",
        "    self.class_dim = 64\n",
        "    self.num_classes = 100\n",
        "    self.reconstruction_coef = 2\n",
        "    self.reverse_cycle_coef = 10.\n",
        "    self.kl_divergence_coef = 3.\n",
        "    self.beta_1 = 0.9\n",
        "    self.beta_2 = 0.999\n",
        "    self.log_file = 'log.txt'\n",
        "    self.image_size = 60\n",
        "    self.load_saved = False\n",
        "    self.encoder_save = 'encoder'\n",
        "    self.decoder_save = 'decoder'\n",
        "    self.start_epoch = 0\n",
        "    self.end_epoch = 1\n",
        "\n",
        "FLAGS = ags()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8M9K3pWHIbQt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "transform_config = Compose([ToTensor()])\n",
        "\n",
        "\n",
        "def mse_loss(input, target):\n",
        "    return torch.sum((input - target).pow(2)) / input.data.nelement()\n",
        "\n",
        "\n",
        "def l1_loss(input, target):\n",
        "    return torch.sum(torch.abs(input - target)) / input.data.nelement()\n",
        "\n",
        "\n",
        "def reparameterize(training, mu, logvar):\n",
        "    if training:\n",
        "        std = logvar.mul(0.5).exp_()\n",
        "        eps = Variable(std.data.new(std.size()).normal_())\n",
        "        return eps.mul(std).add_(mu)\n",
        "    else:\n",
        "        return mu\n",
        "\n",
        "\n",
        "def weights_init(layer):\n",
        "    if isinstance(layer, nn.Conv2d):\n",
        "        layer.weight.data.normal_(0.0, 0.05)\n",
        "        layer.bias.data.zero_()\n",
        "    elif isinstance(layer, nn.BatchNorm2d):\n",
        "        layer.weight.data.normal_(1.0, 0.02)\n",
        "        layer.bias.data.zero_()\n",
        "    elif isinstance(layer, nn.Linear):\n",
        "        layer.weight.data.normal_(0.0, 0.05)\n",
        "        layer.bias.data.zero_()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S2uHESroOruW",
        "colab_type": "text"
      },
      "source": [
        "**For classification**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tg5tZQAgOqBq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train2(FLAGS):\n",
        "    \"\"\"\n",
        "    model definition\n",
        "    \"\"\"\n",
        "    encoder = Encoder(style_dim=FLAGS.style_dim, class_dim=FLAGS.class_dim)\n",
        "    encoder.apply(weights_init)\n",
        "    \n",
        "\n",
        "    decoder = Decoder(style_dim=FLAGS.style_dim, class_dim=FLAGS.class_dim)\n",
        "    decoder.apply(weights_init)\n",
        "\n",
        "    \n",
        "    # load saved models if load_saved flag is true\n",
        "    if FLAGS.load_saved:\n",
        "        encoder.load_state_dict(torch.load(os.path.join('checkpoints', FLAGS.encoder_save)))\n",
        "        decoder.load_state_dict(torch.load(os.path.join('checkpoints', FLAGS.decoder_save)))\n",
        "\n",
        "    \"\"\"\n",
        "    variable definition\n",
        "    \"\"\"\n",
        "\n",
        "    X_1 = torch.FloatTensor(FLAGS.batch_size, FLAGS.num_channels, FLAGS.image_size, FLAGS.image_size)\n",
        "    X_2 = torch.FloatTensor(FLAGS.batch_size, FLAGS.num_channels, FLAGS.image_size, FLAGS.image_size)\n",
        "    X_3 = torch.FloatTensor(FLAGS.batch_size, FLAGS.num_channels, FLAGS.image_size, FLAGS.image_size)\n",
        "\n",
        "    style_latent_space = torch.FloatTensor(FLAGS.batch_size, FLAGS.style_dim)\n",
        "\n",
        "    \"\"\"\n",
        "    loss definitions\n",
        "    \"\"\"\n",
        "    cross_entropy_loss = nn.CrossEntropyLoss()\n",
        "\n",
        "    '''\n",
        "    add option to run on GPU\n",
        "    '''\n",
        "    if FLAGS.cuda:\n",
        "        encoder.cuda()\n",
        "        decoder.cuda()\n",
        "\n",
        "        cross_entropy_loss.cuda()\n",
        "\n",
        "        X_1 = X_1.cuda()\n",
        "        X_2 = X_2.cuda()\n",
        "        X_3 = X_3.cuda()\n",
        "\n",
        "        style_latent_space = style_latent_space.cuda()\n",
        "\n",
        "    \"\"\"\n",
        "    optimizer and scheduler definition\n",
        "    \"\"\"\n",
        "    auto_encoder_optimizer = optim.Adam(\n",
        "        list(encoder.parameters()) + list(decoder.parameters()),\n",
        "        lr=FLAGS.initial_learning_rate,\n",
        "        betas=(FLAGS.beta_1, FLAGS.beta_2)\n",
        "    )\n",
        "\n",
        "    reverse_cycle_optimizer = optim.Adam(\n",
        "        list(encoder.parameters()),\n",
        "        lr=FLAGS.initial_learning_rate,\n",
        "        betas=(FLAGS.beta_1, FLAGS.beta_2)\n",
        "    )\n",
        "\n",
        "    # divide the learning rate by a factor of 10 after 80 epochs\n",
        "    auto_encoder_scheduler = optim.lr_scheduler.StepLR(auto_encoder_optimizer, step_size=80, gamma=0.1)\n",
        "    reverse_cycle_scheduler = optim.lr_scheduler.StepLR(reverse_cycle_optimizer, step_size=80, gamma=0.1)\n",
        "\n",
        "    \"\"\"\n",
        "    training\n",
        "    \"\"\"\n",
        "    if torch.cuda.is_available() and not FLAGS.cuda:\n",
        "        print(\"WARNING: You have a CUDA device, so you should probably run with --cuda\")\n",
        "\n",
        "    if not os.path.exists('checkpoints'):\n",
        "        os.makedirs('checkpoints')\n",
        "\n",
        "    if not os.path.exists('reconstructed_images'):\n",
        "        os.makedirs('reconstructed_images')\n",
        "\n",
        "    # load_saved is false when training is started from 0th iteration\n",
        "    if not FLAGS.load_saved:\n",
        "        with open(FLAGS.log_file, 'w') as log:\n",
        "            log.write('Epoch\\tIteration\\tReconstruction_loss\\tKL_divergence_loss\\tReverse_cycle_loss\\n')\n",
        "\n",
        "    # load data set and create data loader instance\n",
        "    print('Loading stripes paired dataset...')\n",
        "    \n",
        "    loader = cycle(DataLoader(dataset, batch_size=FLAGS.batch_size, shuffle=True, num_workers=0, drop_last=True))\n",
        "\n",
        "    # initialize summary writer\n",
        "    writer = SummaryWriter()\n",
        "    for epoch in range(FLAGS.start_epoch, FLAGS.end_epoch):\n",
        "        print('')\n",
        "        # print('Epoch #' + str(epoch) + '..........................................................................')\n",
        "        print(epoch)\n",
        "        # update the learning rate scheduler\n",
        "        auto_encoder_scheduler.step()\n",
        "        reverse_cycle_scheduler.step()\n",
        "\n",
        "        for iteration in range(int(len(dataset) / FLAGS.batch_size)):\n",
        "            # A. run the auto-encoder reconstruction\n",
        "            image_batch_1, image_batch_2, batch_labels = next(loader)\n",
        "            auto_encoder_optimizer.zero_grad()\n",
        "            X_1.copy_(image_batch_1)\n",
        "            X_2.copy_(image_batch_2)\n",
        "\n",
        "            style_mu_1, style_logvar_1, class_latent_space_1 = encoder(Variable(X_1))\n",
        "            style_latent_space_1 = reparameterize(training=True, mu=style_mu_1, logvar=style_logvar_1)\n",
        "\n",
        "            #\n",
        "            #\n",
        "            #\n",
        "            #\n",
        "            #\n",
        "            if epoch == FLAGS.end_epoch - 1:\n",
        "\n",
        "              if iteration == 0:\n",
        "                temp_labels = batch_labels\n",
        "                temp_class_latent_space = class_latent_space_1\n",
        "                temp_style_latent_space = style_latent_space_1\n",
        "                true_image_batches = image_batch_1\n",
        "              else:\n",
        "                temp_labels = torch.cat((temp_labels, batch_labels))\n",
        "                temp_class_latent_space = torch.cat((temp_class_latent_space, class_latent_space_1))\n",
        "                temp_style_latent_space = torch.cat((temp_style_latent_space, style_latent_space_1))\n",
        "                true_image_batches = torch.cat((true_image_batches, image_batch_1))\n",
        "            kl_divergence_loss_1 = FLAGS.kl_divergence_coef * (\n",
        "                - 0.5 * torch.sum(1 + style_logvar_1 - style_mu_1.pow(2) - style_logvar_1.exp())\n",
        "            )\n",
        "            kl_divergence_loss_1 /= (FLAGS.batch_size * FLAGS.num_channels * FLAGS.image_size * FLAGS.image_size)\n",
        "            kl_divergence_loss_1.backward(retain_graph=True)\n",
        "\n",
        "            style_mu_2, style_logvar_2, class_latent_space_2 = encoder(Variable(X_2))\n",
        "            style_latent_space_2 = reparameterize(training=True, mu=style_mu_2, logvar=style_logvar_2)\n",
        "\n",
        "            kl_divergence_loss_2 = FLAGS.kl_divergence_coef * (\n",
        "                - 0.5 * torch.sum(1 + style_logvar_2 - style_mu_2.pow(2) - style_logvar_2.exp())\n",
        "            )\n",
        "            kl_divergence_loss_2 /= (FLAGS.batch_size * FLAGS.num_channels * FLAGS.image_size * FLAGS.image_size)\n",
        "            kl_divergence_loss_2.backward(retain_graph=True)\n",
        "\n",
        "            reconstructed_X_1 = decoder(style_latent_space_1, class_latent_space_2)\n",
        "            reconstructed_X_2 = decoder(style_latent_space_2, class_latent_space_1)\n",
        "\n",
        "            reconstruction_error_1 = FLAGS.reconstruction_coef * mse_loss(reconstructed_X_1, Variable(X_1))\n",
        "            reconstruction_error_1.backward(retain_graph=True)\n",
        "\n",
        "            reconstruction_error_2 = FLAGS.reconstruction_coef * mse_loss(reconstructed_X_2, Variable(X_2))\n",
        "            reconstruction_error_2.backward()\n",
        "\n",
        "            reconstruction_error = (reconstruction_error_1 + reconstruction_error_2) / FLAGS.reconstruction_coef\n",
        "            kl_divergence_error = (kl_divergence_loss_1 + kl_divergence_loss_2) / FLAGS.kl_divergence_coef\n",
        "\n",
        "            auto_encoder_optimizer.step()\n",
        "\n",
        "            # B. reverse cycle\n",
        "            image_batch_1, _, __ = next(loader)\n",
        "            image_batch_2, _, __ = next(loader)\n",
        "\n",
        "            reverse_cycle_optimizer.zero_grad()\n",
        "\n",
        "            X_1.copy_(image_batch_1)\n",
        "            X_2.copy_(image_batch_2)\n",
        "\n",
        "            style_latent_space.normal_(0., 1.)\n",
        "\n",
        "            _, __, class_latent_space_1 = encoder(Variable(X_1))\n",
        "            _, __, class_latent_space_2 = encoder(Variable(X_2))\n",
        "\n",
        "            reconstructed_X_1 = decoder(Variable(style_latent_space), class_latent_space_1.detach())\n",
        "            reconstructed_X_2 = decoder(Variable(style_latent_space), class_latent_space_2.detach())\n",
        "\n",
        "            style_mu_1, style_logvar_1, _ = encoder(reconstructed_X_1)\n",
        "            style_latent_space_1 = reparameterize(training=False, mu=style_mu_1, logvar=style_logvar_1)\n",
        "\n",
        "            style_mu_2, style_logvar_2, _ = encoder(reconstructed_X_2)\n",
        "            style_latent_space_2 = reparameterize(training=False, mu=style_mu_2, logvar=style_logvar_2)\n",
        "\n",
        "            reverse_cycle_loss = FLAGS.reverse_cycle_coef * l1_loss(style_latent_space_1, style_latent_space_2)\n",
        "            reverse_cycle_loss.backward()\n",
        "            reverse_cycle_loss /= FLAGS.reverse_cycle_coef\n",
        "\n",
        "            reverse_cycle_optimizer.step()\n",
        "\n",
        "            if (iteration + 1) % 10 == 0:\n",
        "                print('')\n",
        "                print('Epoch #' + str(epoch))\n",
        "                print('Iteration #' + str(iteration))\n",
        "\n",
        "                print('')\n",
        "                print('Reconstruction loss: ' + str(reconstruction_error.data.storage().tolist()[0]))\n",
        "                print('KL-Divergence loss: ' + str(kl_divergence_error.data.storage().tolist()[0]))\n",
        "                print('Reverse cycle loss: ' + str(reverse_cycle_loss.data.storage().tolist()[0]))\n",
        "\n",
        "            # write to log\n",
        "            with open(FLAGS.log_file, 'a') as log:\n",
        "                log.write('{0}\\t{1}\\t{2}\\t{3}\\t{4}\\n'.format(\n",
        "                    epoch,\n",
        "                    iteration,\n",
        "                    reconstruction_error.data.storage().tolist()[0],\n",
        "                    kl_divergence_error.data.storage().tolist()[0],\n",
        "                    reverse_cycle_loss.data.storage().tolist()[0]\n",
        "                ))\n",
        "\n",
        "            # write to tensorboard\n",
        "            writer.add_scalar('Reconstruction loss', reconstruction_error.data.storage().tolist()[0],\n",
        "                              epoch * (int(len(dataset) / FLAGS.batch_size) + 1) + iteration)\n",
        "            writer.add_scalar('KL-Divergence loss', kl_divergence_error.data.storage().tolist()[0],\n",
        "                              epoch * (int(len(dataset) / FLAGS.batch_size) + 1) + iteration)\n",
        "            writer.add_scalar('Reverse cycle loss', reverse_cycle_loss.data.storage().tolist()[0],\n",
        "                              epoch * (int(len(dataset) / FLAGS.batch_size) + 1) + iteration)\n",
        "\n",
        "        # save model after every 5 epochs\n",
        "        if (epoch + 1) % 5 == 0 or (epoch + 1) == FLAGS.end_epoch:\n",
        "            torch.save(encoder.state_dict(), os.path.join('checkpoints', FLAGS.encoder_save))\n",
        "            torch.save(decoder.state_dict(), os.path.join('checkpoints', FLAGS.decoder_save))\n",
        "\n",
        "            \"\"\"\n",
        "            save reconstructed images and style swapped image generations to check progress\n",
        "            \"\"\"\n",
        "            image_batch_1, image_batch_2, _ = next(loader)\n",
        "            image_batch_3, _, __ = next(loader)\n",
        "\n",
        "            X_1.copy_(image_batch_1)\n",
        "            X_2.copy_(image_batch_2)\n",
        "            X_3.copy_(image_batch_3)\n",
        "\n",
        "            style_mu_1, style_logvar_1, _ = encoder(Variable(X_1))\n",
        "            _, __, class_latent_space_2 = encoder(Variable(X_2))\n",
        "            style_mu_3, style_logvar_3, _ = encoder(Variable(X_3))\n",
        "\n",
        "            style_latent_space_1 = reparameterize(training=False, mu=style_mu_1, logvar=style_logvar_1)\n",
        "            style_latent_space_3 = reparameterize(training=False, mu=style_mu_3, logvar=style_logvar_3)\n",
        "\n",
        "            reconstructed_X_1_2 = decoder(style_latent_space_1, class_latent_space_2)\n",
        "            reconstructed_X_3_2 = decoder(style_latent_space_3, class_latent_space_2)\n",
        "\n",
        "            # save input image batch\n",
        "            # image_batch = np.transpose(X_1.cpu().numpy(), (0, 2, 3, 1))\n",
        "            # # image_batch = np.concatenate((image_batch, image_batch, image_batch), axis=3)\n",
        "            # imshow_grid(image_batch, name=str(epoch) + '_original', save=True)\n",
        "\n",
        "            # # save reconstructed batch\n",
        "            # reconstructed_x = np.transpose(reconstructed_X_1_2.cpu().data.numpy(), (0, 2, 3, 1))\n",
        "            # # reconstructed_x = np.concatenate((reconstructed_x, reconstructed_x, reconstructed_x), axis=3)\n",
        "            # imshow_grid(reconstructed_x, name=str(epoch) + '_target', save=True)\n",
        "\n",
        "            # style_batch = np.transpose(X_3.cpu().numpy(), (0, 2, 3, 1))\n",
        "            # # style_batch = np.concatenate((style_batch, style_batch, style_batch), axis=3)\n",
        "            # imshow_grid(style_batch, name=str(epoch) + '_style', save=True)\n",
        "\n",
        "            # # save style swapped reconstructed batch\n",
        "            # reconstructed_style = np.transpose(reconstructed_X_3_2.cpu().data.numpy(), (0, 2, 3, 1))\n",
        "            # # reconstructed_style = np.concatenate((reconstructed_style, reconstructed_style, reconstructed_style), axis=3)\n",
        "            # imshow_grid(reconstructed_style, name=str(epoch) + '_style_target', save=True)\n",
        "    return temp_labels, temp_class_latent_space, temp_style_latent_space, true_image_batches"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rYMmJSbofWQZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "FLAGS.load_saved = True\n",
        "l, c, s, t = train2(FLAGS)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zoHHhPjjPqwe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(type(l), type(c), type(s), type(t))\n",
        "print(l.shape, c.shape, s.shape, t.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0MI1JBWBXPRp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "torch.save(l, 'l.pt')\n",
        "torch.save(c, 'c.pt')\n",
        "torch.save(s, 's.pt')\n",
        "torch.save(t, 't.pt')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HcqZw1chP2FC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "l = torch.load('l.pt')\n",
        "c = torch.load('c.pt')\n",
        "s = torch.load('s.pt')\n",
        "t = torch.load('t.pt')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r9E95nsSaQgl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "a = l.shape[0]\n",
        "b = int(l.shape[0]*0.7)\n",
        "y_train, y_test = torch.split(l, [b, a - b])\n",
        "class_train, class_test = torch.split(c, [b, a - b])\n",
        "style_train, style_test = torch.split(s, [b, a - b])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0gO6zpsvbMm4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(y_train.shape, y_test.shape)\n",
        "print(class_train.shape, class_test.shape)\n",
        "print(style_train.shape, style_test.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MYhSAOJFbi8A",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import sklearn\n",
        "from sklearn import svm\n",
        "clf = svm.SVC()\n",
        "clf.fit(np.array(style_train.detach().cpu()), np.array(y_train.detach().cpu()))\n",
        "train_pred = clf.predict(np.array(style_train.detach().cpu()))\n",
        "train_acc = sklearn.metrics.accuracy_score(np.array(y_train.detach().cpu()), train_pred)\n",
        "print('train acc' + str(train_acc))\n",
        "pred = clf.predict(np.array(style_test.detach().cpu()))\n",
        "acc = sklearn.metrics.accuracy_score(np.array(y_test.detach().cpu()), pred)\n",
        "print('test acc' +str(acc))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SjZouz946pRg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(clf.score(np.array(style_train.detach().cpu()), np.array(y_train.detach().cpu())))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vaicTQ5Vg2fa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "clf2 = svm.SVC()\n",
        "clf2.fit(np.array(class_train.detach().cpu()), np.array(y_train.detach().cpu()))\n",
        "train_pred2 = clf2.predict(np.array(class_train.detach().cpu()))\n",
        "train_acc = sklearn.metrics.accuracy_score(np.array(y_train.detach().cpu()), train_pred)\n",
        "pred2 = clf2.predict(np.array(class_test.detach().cpu()))\n",
        "acc2 = sklearn.metrics.accuracy_score(np.array(y_test.detach().cpu()), pred2)\n",
        "print(acc2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BRPuvKNY6T3Z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(train_acc)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m8lnrxS65tE-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "X = c.detach().cpu()\n",
        "y = s.detach().cpu()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7kQC4eLbPRmK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train_prediction(X, y, l):\n",
        "  model = nn.Sequential(nn.Linear(X.shape[1], X.shape[1]),\n",
        "                        nn.ReLU(),\n",
        "                        nn.Linear(X.shape[1], X.shape[1]),\n",
        "                        nn.ReLU(),\n",
        "                        nn.Linear(X.shape[1], X.shape[1]),\n",
        "                        nn.ReLU(),\n",
        "                        nn.Linear(X.shape[1], y.shape[1]))\n",
        "  # criterion = nn.MSELoss()\n",
        "  criterion = nn.L1Loss()\n",
        "  optimizer = optim.SGD(model.parameters(), lr = l)\n",
        "  epochs = 100\n",
        "  model_loss = []\n",
        "  for e in range(epochs):\n",
        "    running_loss = 0\n",
        "    for i in range(X.shape[0]):\n",
        "    \n",
        "        # Training pass\n",
        "        optimizer.zero_grad()\n",
        "        \n",
        "        output = model(X[i])\n",
        "        loss = criterion(output, y[i])\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        \n",
        "        running_loss += loss.item()\n",
        "    model_loss.append(running_loss/ X.shape[0])\n",
        "    print(e, ' # ',running_loss/X.shape[0])\n",
        "  return model(X), model_loss"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Diq-SNnePiEV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "a, loss1 = train_prediction(X, y, 0.01)\n",
        "import matplotlib.pyplot as plt\n",
        "plt.plot(loss1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ekARSKoZdX9_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "b, loss2 = train_prediction(y, X, 0.001)\n",
        "plt.plot(loss2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b_xQxlNYQVKs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "decoder = Decoder(style_dim=FLAGS.style_dim, class_dim=FLAGS.class_dim)\n",
        "decoder.load_state_dict(torch.load(os.path.join('checkpoints', FLAGS.decoder_save)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LVLlHLIr342_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "reconstructed_X_1 = decoder(a[:16], b[:16])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DEu0tTpnkbNJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(reconstructed_X_1.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RKYRHPotkkqP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "reconstructed_X = np.transpose(reconstructed_X_1.cpu().data.numpy(), (0, 3, 2, 1))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vSVljwp6k7pN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for i in range(16):\n",
        "  plt.imshow(reconstructed_X[i])\n",
        "  plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0QsOjAl-3fLD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "k = np.transpose(t, (0, 3, 2, 1))\n",
        "for i in range(16):\n",
        "  plt.imshow(k[i])\n",
        "  plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M_QUIDdL3v32",
        "colab_type": "text"
      },
      "source": [
        "**Style Transfer Grids**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gp7j0PBDSrRL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def plot_style_grid_matrix(FLAGS):\n",
        "  encoder = Encoder(style_dim=FLAGS.style_dim, class_dim=FLAGS.class_dim)\n",
        "  decoder = Decoder(style_dim=FLAGS.style_dim, class_dim=FLAGS.class_dim)\n",
        "  \n",
        "  X_1 = torch.FloatTensor(FLAGS.batch_size, FLAGS.num_channels, FLAGS.image_size, FLAGS.image_size)\n",
        "  X_2 = torch.FloatTensor(FLAGS.batch_size, FLAGS.num_channels, FLAGS.image_size, FLAGS.image_size)\n",
        " \n",
        "  # load saved models if load_saved flag is true\n",
        "  if FLAGS.load_saved:\n",
        "      encoder.load_state_dict(torch.load(os.path.join('checkpoints', FLAGS.encoder_save)))\n",
        "      decoder.load_state_dict(torch.load(os.path.join('checkpoints', FLAGS.decoder_save)))\n",
        "\n",
        "  if FLAGS.cuda:\n",
        "        encoder.cuda()\n",
        "        decoder.cuda()        \n",
        "\n",
        "        X_1 = X_1.cuda()\n",
        "        X_2 = X_2.cuda()\n",
        "        \n",
        "  \n",
        " \n",
        "  image_batch_1, _, _ = next(test_loader)\n",
        "  image_batch_2, _, _ = next(test_loader)\n",
        "  \n",
        "  \n",
        "  X_1.copy_(image_batch_1)\n",
        "  X_2.copy_(image_batch_2)\n",
        "  \n",
        "\n",
        "  style_mu_1, style_logvar_1, _ = encoder(Variable(X_1))\n",
        "  _, __, class_latent_space_2 = encoder(Variable(X_2))\n",
        "  \n",
        "\n",
        "  style_latent_space_1 = reparameterize(training=False, mu=style_mu_1, logvar=style_logvar_1)\n",
        " \n",
        "  # print(np.array(style_latent_space_1.detach().cpu()).shape, np.array(class_latent_space_2.detach().cpu()).shape)\n",
        "  style_latent_nparray = np.array(style_latent_space_1.detach().cpu())\n",
        "  # print(arr[0] == arr[1], arr[0] == arr[2])\n",
        "\n",
        "  # reconstructed_X_1_2 = decoder(style_latent_space_1, class_latent_space_2)\n",
        "  # reconstructed_X_3_2 = decoder(style_latent_space_3, class_latent_space_2)\n",
        "  k = style_latent_nparray.shape[0]\n",
        "  reconstructed_images = np.zeros((k, 60, 60, 3))\n",
        "  for i in range(0, k):\n",
        "    style_latent_temp = np.zeros((k, FLAGS.style_dim),dtype = 'float32')\n",
        "    for j in range(0, k):\n",
        "      style_latent_temp[j] = style_latent_nparray[i]\n",
        "\n",
        "    \n",
        "    if FLAGS.cuda:\n",
        "      style_latent_temp = torch.from_numpy(style_latent_temp)\n",
        "      style_latent_temp = style_latent_temp.cuda()\n",
        "    reconstructed_x = decoder(style_latent_temp, class_latent_space_2)\n",
        "    reconstructed_xx = np.transpose(reconstructed_x.cpu().data.numpy(), (0, 3, 2, 1))\n",
        "    if i == 0:\n",
        "      reconstructed_images = np.copy(reconstructed_xx)\n",
        "    else:\n",
        "      reconstructed_images = np.concatenate([reconstructed_images, reconstructed_xx], 0)\n",
        "  \n",
        "  image_batch_2 = np.array(image_batch_2)\n",
        "  print(type(image_batch_2))\n",
        "  image_batch_2 = np.transpose(image_batch_2, (0, 3, 2, 1))\n",
        "  reconstructed_images = np.concatenate([image_batch_2, reconstructed_images], 0)\n",
        "  print(reconstructed_images.shape)\n",
        "  image_batch_1 = np.transpose(image_batch_1.numpy(), (0, 3, 2, 1))\n",
        "  blank_img = np.zeros((60, 60, 3))\n",
        "  image_list = []\n",
        "  image_list.append(blank_img)\n",
        "  j = 0\n",
        "  for i in range(reconstructed_images.shape[0]):\n",
        "    if i % 8 == 0 and j != 8 and i != 0:\n",
        "      image_list.append(image_batch_1[j])\n",
        "      \n",
        "      j += 1\n",
        "    image_list.append(reconstructed_images[i])\n",
        "  print(len(image_list))\n",
        "  fig = plt.figure(figsize=(12., 12.))\n",
        "  grid = ImageGrid(fig, 111,  # similar to subplot(111)\n",
        "                  nrows_ncols=(9, 9),  # creates 2x2 grid of axes\n",
        "                  axes_pad=0.01,  # pad between axes in inch.\n",
        "                  )\n",
        "  # print(label1)\n",
        "  for ax, im in zip(grid, image_list):\n",
        "      # Iterating over the grid returns the Axes.\n",
        "      ax.imshow(im)\n",
        "\n",
        "  plt.show()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0qNV42Lm28XH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "F2 = ags()\n",
        "F2.load_saved = True\n",
        "F2.batch_size = 8\n",
        "plot_style_grid_matrix(F2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y_c0qSfl29Xj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def linear_interpolation_line(FLAGS):\n",
        "  encoder = Encoder(style_dim=FLAGS.style_dim, class_dim=FLAGS.class_dim)   \n",
        "  decoder = Decoder(style_dim=FLAGS.style_dim, class_dim=FLAGS.class_dim)\n",
        "  \n",
        "  X_1 = torch.FloatTensor(FLAGS.batch_size, FLAGS.num_channels, FLAGS.image_size, FLAGS.image_size)\n",
        "  X_2 = torch.FloatTensor(FLAGS.batch_size, FLAGS.num_channels, FLAGS.image_size, FLAGS.image_size)\n",
        " \n",
        "  # load saved models if load_saved flag is true\n",
        "  if FLAGS.load_saved:\n",
        "      encoder.load_state_dict(torch.load(os.path.join('checkpoints', FLAGS.encoder_save)))\n",
        "      decoder.load_state_dict(torch.load(os.path.join('checkpoints', FLAGS.decoder_save)))\n",
        " \n",
        "  if FLAGS.cuda:\n",
        "        encoder.cuda()\n",
        "        decoder.cuda()\n",
        "        X_1 = X_1.cuda()\n",
        "        X_2 = X_2.cuda() \n",
        "  image_batch_1, _ ,_  = next(test_loader)\n",
        "  image_batch_2, _, _ = next(test_loader)\n",
        "  \n",
        "  X_1.copy_(image_batch_1)\n",
        "  X_2.copy_(image_batch_2)\n",
        "\n",
        "  style_mu_1, style_logvar_1, class_latent_space_1 = encoder(Variable(X_1))\n",
        "  style_mu_2, style_logvar_2, class_latent_space_2 = encoder(Variable(X_2))\n",
        "  \n",
        "  style_latent_space_1 = reparameterize(training= False, mu = style_mu_1, logvar = style_logvar_1)\n",
        "  style_latent_space_2 = reparameterize(training = False, mu = style_mu_2, logvar = style_logvar_2)\n",
        "  image_batch_1 = np.transpose(image_batch_1.numpy(), (0, 3, 2, 1))\n",
        "  image_batch_2 = np.transpose(image_batch_2.numpy(), (0, 3, 2, 1))\n",
        "  style_latent_nparray1 = np.array(style_latent_space_1.detach().cpu())\n",
        "  style_latent_nparray2 = np.array(style_latent_space_2.detach().cpu())\n",
        "  class_latent_nparray2 = np.array(class_latent_space_2.detach().cpu())\n",
        "  class_latent_nparray1 = np.array(class_latent_space_1.detach().cpu())\n",
        "  reconstructed_images = np.zeros((8, 8, 60, 60, 3), dtype = 'float32')\n",
        "  class_latent_spaces = np.zeros((8, 8, 1, 64), dtype = 'float32')\n",
        "  style_latent_spaces = np.zeros((8, 8, 1, 64), dtype = 'float32')\n",
        "  steps = 8\n",
        "  for k in range(8):\n",
        "    new_class_latent_space = (class_latent_nparray2 - class_latent_nparray1) *  k / steps + class_latent_nparray1\n",
        "    style_latent_spaces[0][k] = style_latent_nparray1\n",
        "    class_latent_spaces[0][k] = new_class_latent_space\n",
        "    # x = torch.from_numpy(new_class_latent_space)\n",
        "    # class_latent_space = x.cuda()\n",
        "\n",
        "    \n",
        "    # recons = decoder(style_latent_space_1, class_latent_space)\n",
        "    # print(recons)\n",
        "    # reconstructed_images[k] = np.transpose(recons.detach().cpu().data.numpy(), (0, 3, 2, 1))[0]\n",
        "  for k in range(7, -1, -1):\n",
        "    new_class_spaces = (class_latent_nparray1 - class_latent_nparray2) * (7 - k)/steps + class_latent_nparray2\n",
        "    style_latent_spaces[7][k] = style_latent_nparray2\n",
        "    class_latent_spaces[7][k] = new_class_spaces\n",
        "  steps = 6\n",
        "  for k in range(0, 8):\n",
        "    for j in range(1, 7):\n",
        "      new_style_spaces = (style_latent_spaces[7][k] - style_latent_spaces[0][k])  * j / steps + style_latent_spaces[0][k]\n",
        "      style_latent_spaces[j][k] = new_style_spaces\n",
        "      class_latent_spaces[j][k] = class_latent_spaces[j][7]\n",
        "  for j in range(0, 8):\n",
        "    for k in range(0, 8):\n",
        "      # print(np.shape(style_latent_spaces[j][k]))\n",
        "      stl = torch.from_numpy(style_latent_spaces[j][k])\n",
        "      cls = torch.from_numpy(class_latent_spaces[j][k])\n",
        "      stl = stl.cuda()\n",
        "      cls = cls.cuda()\n",
        "      img = decoder(stl, cls)\n",
        "      reconstructed_images[j][k] = np.transpose(img.detach().cpu().data.numpy(), (0, 3, 2, 1))[0]\n",
        "      # print(np.shape(reconstructed_images[j][k]))\n",
        "  # print(np.shape(image_batch_1[0]))\n",
        "  reconstructed_images[0][0] = image_batch_1[0]\n",
        "  reconstructed_images[7][7] = image_batch_2[0]\n",
        "\n",
        "\n",
        "  image_list = []\n",
        "  \n",
        "  for i in range(reconstructed_images.shape[0]):\n",
        "    for j in range(reconstructed_images.shape[1]):\n",
        "      image_list.append(reconstructed_images[i][j])\n",
        "    \n",
        "  # print(len(image_list))\n",
        "  fig = plt.figure(figsize=(10., 10.))\n",
        "  grid = ImageGrid(fig, 111,  # similar to subplot(111)\n",
        "                  nrows_ncols=(8, 8),  # creates 2x2 grid of axes\n",
        "                  axes_pad=0.01,  # pad between axes in inch.\n",
        "                  )\n",
        "  # print(label1)\n",
        "  for ax, im in zip(grid, image_list):\n",
        "      # Iterating over the grid returns the Axes.\n",
        "      ax.imshow(im)\n",
        "\n",
        "  plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kkuqQHlA3Eji",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "F1 = ags()\n",
        "F1.load_saved = True\n",
        "F1.batch_size = 1\n",
        "linear_interpolation_line(F1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "57bGnbyb7UnS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}